---
layout: post
title: "전이 학습(트랜스퍼 러닝): 사전 학습된 AI 모델을 활용한 효율적인 학습 방법"
category: ai
tags: [transfer-learning, machine-learning, bert, nlp, pre-training]
excerpt: "이미 학습된 모델을 재학습하여 새로운 태스크에 적용하는 전이 학습의 개념과 업스트림/다운스트림 테스크에 대해 알아봅니다."
---

## 전이 학습이란?

**전이 학습(Transfer Learning)**이란 이미 학습된 모델을 재학습하는 방법입니다. 예를 들어, 사전 학습된 BERT 모델을 메일 데이터로 스팸 분류 모델로 만드는 것이 해당됩니다.

처음부터 모델을 학습시키는 것보다 이미 대규모 데이터로 학습된 모델을 활용하면 **시간과 리소스를 절약**하면서도 높은 성능을 얻을 수 있습니다.

---

## 업스트림 테스크 (Pre-train)

업스트림 테스크는 **선행 학습 단계**를 의미합니다. 이 단계에서 모델은 대규모 데이터로 기본적인 언어 이해 능력을 학습합니다.

### 주요 학습 방법

- **다음 단어 맞추기**: 문장의 일부를 보고 다음에 올 단어를 예측
- **빈칸 채우기**: 문장에서 마스킹된 단어를 예측 (BERT의 MLM)

이러한 방법으로 모델은 언어의 문법, 의미, 맥락을 이해하게 됩니다.

---

## 다운스트림 테스크

다운스트림 테스크는 사전 학습된 모델을 **특정 목적**에 맞게 파인튜닝하는 단계입니다.

### 1. 문서 분류 (Document Classification)

자연어 입력을 특정 범주로 분류하는 태스크입니다.

- **예시**: 감정 분석 (긍정, 중립, 부정)
- **출력**: 각 범주에 대한 확률값

### 2. 자연어 추론 (Natural Language Inference)

두 문장 간의 관계를 판단하는 태스크입니다.

- **예시**: 전제 문장과 가설 문장의 관계가 참인지 거짓인지 판단
- **출력**: 함의(entailment), 모순(contradiction), 중립(neutral) 확률

### 3. 개체명 인식 (Named Entity Recognition)

문서나 문장에서 특정 개체를 인식하고 분류하는 태스크입니다.

- **예시**: 인명, 지명, 기관명 등 추출
- **출력**: 각 토큰에 대한 개체 범주 확률값

### 4. 질의응답 (Question Answering)

질문과 지문을 입력받아 답변을 도출하는 태스크입니다.

- **예시**: SQuAD 데이터셋 기반 질의응답
- **출력**: 지문에서 정답에 해당하는 텍스트 범위

---

## 전이 학습의 장점

1. **학습 시간 단축**: 처음부터 학습하는 것보다 훨씬 빠름
2. **적은 데이터로 학습 가능**: 대규모 데이터가 없어도 좋은 성능 달성
3. **높은 성능**: 사전 학습된 지식을 활용하여 더 나은 결과 도출

---

## 참고 자료

- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)
- [Transfer Learning in NLP](https://www.tensorflow.org/tutorials/text/transfer_learning_hub)
